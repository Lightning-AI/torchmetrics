# Create and test a Python package on multiple PyTorch versions.

trigger:
  tags:
    include:
      - "*"
  branches:
    include:
      - master
      - release/*
      - refs/tags/*
pr:
  - master
  - release/*

jobs:
  - job: unitest_GPU
    strategy:
      matrix:
        "PyTorch | old":
          # Torch does not have build wheels with old Torch versions for newer CUDA
          docker-image: "nvidia/cuda:11.1.1-cudnn8-devel-ubuntu20.04"
          agent-pool: "lit-rtx-3090"
          torch-ver: "1.8.1"
        "PyTorch | 1.X":
          docker-image: "pytorch/pytorch:1.13.1-cuda11.6-cudnn8-runtime"
          agent-pool: "lit-rtx-3090"
          torch-ver: "1.13.1"
        "PyTorch | 2.X":
          docker-image: "pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime"
          agent-pool: "lit-rtx-3090"
          torch-ver: "2.0.0"
    # how long to run the job before automatically cancelling
    timeoutInMinutes: "120"
    # how much time to give 'run always even if cancelled tasks' before stopping them
    cancelTimeoutInMinutes: "2"

    pool: "$(agent-pool)"

    variables:
      DEVICES: $( python -c 'name = "$(Agent.Name)" ; gpus = name.split("_")[-1] if "_" in name else "0,1"; print(gpus)' )
      # these two caches assume to run repetitively on the same set of machines
      #  see: https://github.com/microsoft/azure-pipelines-agent/issues/4113#issuecomment-1439241481
      TORCH_HOME: "/var/tmp/torch"
      TRANSFORMERS_CACHE: "/var/tmp/huggingface"
      PIP_CACHE_DIR: "/var/tmp/pip"
      # MKL_THREADING_LAYER: "GNU"
      MKL_SERVICE_FORCE_INTEL: 1
      # todo: consider unfreeze for master too
      FREEZE_REQUIREMENTS: 1

    container:
      image: "$(docker-image)"
      options: "--gpus=all --shm-size=8g -v /usr/bin/docker:/tmp/docker:ro  -v /var/tmp:/var/tmp"

    workspace:
      clean: all

    steps:
      - script: |
          set -ex
          container_id=$(head -1 /proc/self/cgroup|cut -d/ -f3)
          echo "container_id=$container_id"
          echo 'debconf debconf/frontend select Noninteractive' | debconf-set-selections
          /tmp/docker exec -t -u 0 $container_id \
            sh -c 'apt-get update && DEBIAN_FRONTEND=noninteractive apt-get -o Dpkg::Options::=--force-confold -y install sudo'
          echo "##vso[task.setvariable variable=CONTAINER_ID]$container_id"
        displayName: "Install Sudo in container (thanks Microsoft!)"

      - script: |
          sudo apt-get update -q --fix-missing
          sudo apt-get install -q -y --no-install-recommends \
            build-essential \
            wget \
            python${PYTHON_VERSION} \
            python${PYTHON_VERSION}-dev \
            python${PYTHON_VERSION}-distutils
          sudo update-alternatives --install /usr/bin/python python /usr/bin/python${PYTHON_VERSION} 1
          wget https://bootstrap.pypa.io/get-pip.py --progress=bar:force:noscroll --no-check-certificate
          python get-pip.py
        env:
          PYTHON_VERSION: "3.8"
        condition: startsWith(variables['docker-image'], 'nvidia/cuda:')
        displayName: "install python & pip"

      - bash: |
          echo "##vso[task.setvariable variable=CUDA_VISIBLE_DEVICES]$(DEVICES)"
          CUDA_version=$(nvcc --version | sed -n 's/^.*release \([0-9]\+\.[0-9]\+\).*$/\1/p')
          CUDA_version_mm="${CUDA_version//'.'/''}"
          echo "##vso[task.setvariable variable=CUDA_VERSION_MM]$CUDA_version_mm"
          echo "##vso[task.setvariable variable=TORCH_URL]https://download.pytorch.org/whl/cu${CUDA_version_mm}/torch_stable.html"
        displayName: "set Env. vars"

      - bash: |
          whoami && id
          lspci | egrep 'VGA|3D'
          whereis nvidia
          nvidia-smi
          echo $CUDA_VISIBLE_DEVICES
          echo $CONTAINER_ID
          echo $TORCH_URL
          python --version
          pip --version
          pip cache dir
          pip list
        displayName: "Image info & NVIDIA"

      - bash: |
          pip install -q packaging wget
          python -m wget https://raw.githubusercontent.com/Lightning-AI/utilities/main/scripts/adjust-torch-versions.py
          python adjust-torch-versions.py requirements.txt $(torch-ver)
          for fpath in `ls requirements/*.txt`; do
              python adjust-torch-versions.py $fpath $(torch-ver)
          done
        displayName: "Adjust versions"

      - bash: |
          set -ex
          sudo apt-get update -qq --fix-missing
          sudo apt-get install -y --no-install-recommends \
            build-essential gcc g++ cmake ffmpeg git libsndfile1 unzip
          # pip install pip -U
          pip install -q "numpy<1.24"  # trying to resolve pesq installation
          pip install . -U -r ./requirements/devel.txt \
            --prefer-binary --find-links=${TORCH_URL}
          pip install mkl-service==2.4.0  # needed for the gpu multiprocessing
          pip list
        displayName: "Install environment"

      - bash: |
          set -e
          python -c "from torch import __version__ as ver ; assert str(ver).split('+')[0] == '$(torch-ver)', f'PyTorch: {ver}'"
          python -c "import torch ; mgpu = torch.cuda.device_count() ; assert mgpu >= 2, f'found GPUs: {mgpu}'"
        displayName: "Sanity check"

      - bash: |
          printf "cache location: $(TORCH_HOME)\n"
          mkdir -p $(TORCH_HOME)  # in case cache was void
          ls -lh $(TORCH_HOME)
          printf "cache location: $(TRANSFORMERS_CACHE)\n"
          mkdir -p $(TRANSFORMERS_CACHE)  # in case cache was void
          ls -lh $(TRANSFORMERS_CACHE)
        displayName: "Show caches"

      - bash: python -m pytest torchmetrics --timeout=240 --durations=50
        env:
          DOCTEST_DOWNLOAD_TIMEOUT: "120"
          SKIP_SLOW_DOCTEST: "1"
        workingDirectory: src
        displayName: "DocTesting"

      - bash: |
          # wget is simpler but does not work on Windows
          python -c "from urllib.request import urlretrieve ; urlretrieve('https://pl-public-data.s3.amazonaws.com/metrics/data.zip', 'data.zip')"
          unzip -o data.zip
          ls -l _data/*
        workingDirectory: tests
        displayName: "Pull testing data from S3"

      - bash: python -m pytest unittests -v --cov=torchmetrics --timeout=240 --durations=500
        env:
          CUDA_LAUNCH_BLOCKING: "1"
        workingDirectory: tests
        displayName: "UnitTesting"

      - bash: |
          python -m coverage report
          python -m coverage xml
          python -m codecov --token=$(CODECOV_TOKEN) --name="GPU-coverage" \
            --commit=$(Build.SourceVersion) --flags=gpu,unittest --env=linux,azure
          ls -l
        workingDirectory: tests
        displayName: "Statistics"

      - bash: |
          set -e
          FILES="*.py"
          for fn in $FILES
          do
            echo "Processing $fn example..."
            python $fn
          done
        workingDirectory: examples
        displayName: "Examples"

      - bash: |
          printf "cache location: $(TRANSFORMERS_CACHE)\n"
          ls -lh $(TRANSFORMERS_CACHE)  # show what was restored...
        displayName: "Show HF artifacts"
