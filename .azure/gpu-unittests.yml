# Create and test a Python package on multiple PyTorch versions.

trigger:
  tags:
    include:
      - "*"
  branches:
    include:
      - master
      - release/*
      - refs/tags/*
# run every month to populate caches
schedules:
  - cron: "0 1 1 * *"
    displayName: Monthly re-build caches
    branches:
      include:
        - master
pr:
  - master
  - release/*

jobs:
  - job: unitest_GPU
    strategy:
      matrix:
        "PyTorch | 2.0 oldest":
          # Torch does not have build wheels with old Torch versions for newer CUDA
          docker-image: "ubuntu22.04-cuda11.8.0-py3.10-torch2.0"
          torch-ver: "2.0"
        "PyTorch | 2.X stable":
          docker-image: "ubuntu22.04-cuda12.4.1-py3.11-torch2.6"
          torch-ver: "2.6"
    # how long to run the job before automatically cancelling
    timeoutInMinutes: "180"
    # how much time to give 'run always even if cancelled tasks' before stopping them
    cancelTimeoutInMinutes: "2"

    pool: "lit-rtx-3090"

    variables:
      DEVICES: $( python -c 'name = "$(Agent.Name)" ; gpus = name.split("_")[-1] if "_" in name else "0,1"; print(gpus)' )
      # these two caches assume to run repetitively on the same set of machines
      #  see: https://github.com/microsoft/azure-pipelines-agent/issues/4113#issuecomment-1439241481
      TORCH_HOME: "/var/tmp/torch"
      TOKENIZERS_PARALLELISM: "false"
      TRANSFORMERS_CACHE: "/var/tmp/hf/transformers"
      HF_HOME: "/var/tmp/hf/home"
      HF_HUB_CACHE: "/var/tmp/hf/hub"
      PIP_CACHE_DIR: "/var/tmp/pip"
      # MKL_THREADING_LAYER: "GNU"
      MKL_SERVICE_FORCE_INTEL: "1"
      TEST_DIRS: "unittests"
      CACHED_REFERENCES: "/var/tmp/cached-references.zip"
      # todo: consider unfreeze for master too
      FREEZE_REQUIREMENTS: 1

    container:
      image: "pytorchlightning/torchmetrics:$(docker-image)"
      options: "--gpus=all --shm-size=8g -v /var/tmp:/var/tmp"

    workspace:
      clean: all

    steps:
      - bash: |
          set -ex
          echo "##vso[task.setvariable variable=CUDA_VISIBLE_DEVICES]$(DEVICES)"
          # nvcc --version  # FIXME!
          CUDA_version=$(nvcc --version | sed -n 's/^.*release \([0-9]\+\.[0-9]\+\).*$/\1/p')
          CUDA_version_mm="${CUDA_version//'.'/''}"
          echo "##vso[task.setvariable variable=CUDA_VERSION_MM]$CUDA_version_mm"
          echo "##vso[task.setvariable variable=TORCH_URL]https://download.pytorch.org/whl/cu${CUDA_version_mm}/torch_stable.html"
          mkdir -p $(TORCH_HOME)
          mkdir -p $(TRANSFORMERS_CACHE)
          mkdir -p $(HF_HOME)
          mkdir -p $(HF_HUB_CACHE)
          mkdir -p $(PIP_CACHE_DIR)
        displayName: "set Env. vars"
      - bash: |
          echo "##vso[task.setvariable variable=ALLOW_SKIP_IF_OUT_OF_MEMORY]1"
          echo "##vso[task.setvariable variable=ALLOW_SKIP_IF_BAD_CONNECTION]1"
        condition: eq(variables['Build.Reason'], 'PullRequest')
        displayName: "set Env. vars for PRs"

      - bash: |
          pip install -q fire pyGithub
          printf "PR: $PR_NUMBER \n"
          focus=$(python .github/assistant.py changed-domains $PR_NUMBER)
          printf "focus: $focus \n"
          echo "##vso[task.setvariable variable=TEST_DIRS]$focus"
        env:
          PR_NUMBER: $(System.PullRequest.PullRequestNumber)
        # run only on PRs
        condition: eq(variables['Build.Reason'], 'PullRequest')
        displayName: "focus diff"

      - bash: |
          whoami && id
          lspci | egrep 'VGA|3D'
          whereis nvidia
          nvidia-smi
          echo $CUDA_VISIBLE_DEVICES
          echo $TORCH_URL
          echo $TEST_DIRS
          python --version
          pip --version
          pip cache dir
          pip list
        displayName: "Image info & NVIDIA"

      - bash: |
          pip install -q packaging
          wget https://raw.githubusercontent.com/Lightning-AI/utilities/main/scripts/adjust-torch-versions.py
          for fpath in `ls requirements/*.txt`; do
              # torch version shall be sourced based on the used docker
              python adjust-torch-versions.py $fpath
          done
        displayName: "Adjust versions"

      - bash: |
          python .github/assistant.py set-oldest-versions
        condition: eq(variables['torch-ver'], '2.0')
        displayName: "Setting oldest versions"

      - bash: |
          pip install . -U -r ./requirements/_devel.txt --prefer-binary --find-links=${TORCH_URL}
        displayName: "Install environment"

      - bash: |
          set -e
          pip list
          python -c "import torch ; mgpu = torch.cuda.device_count() ; assert mgpu >= 2, f'found GPUs: {mgpu}'"
          python -c "from torch import __version__ as ver ; assert '.'.join(str(ver).split('.')[:2]) == '$(torch-ver)', f'PyTorch: installed {ver} but expected $(torch-ver)'"
        displayName: "Sanity check"

      - bash: |
          pip install -q py-tree
          py-tree /var/tmp/torch
          py-tree /var/tmp/hf
        displayName: "Show caches"

      - bash: |
          python -m pytest torchmetrics --cov=torchmetrics \
            --timeout=240 --durations=50 \
            --reruns 2 --reruns-delay 1
          #  --numprocesses=5 --dist=loadfile
        env:
          DOCTEST_DOWNLOAD_TIMEOUT: "180"
          SKIP_SLOW_DOCTEST: "1"
        workingDirectory: "src/"
        timeoutInMinutes: "40"
        displayName: "DocTesting"

      - bash: |
          df -h .
          ls -lh $(CACHED_REFERENCES)
          ls -lh tests/
          # Check if the file references exists
          if [ -f $(CACHED_REFERENCES) ]; then
              # Create a directory if it doesn't already exist
              mkdir -p tests/_cache-references
              # Unzip 'ref.zip' into the directory inside tests folder
              unzip -q $(CACHED_REFERENCES) -d tests/_cache-references
              ls -lh tests/_cache-references/
          else
              echo "The file '$(CACHED_REFERENCES)' does not exist."
          fi
        timeoutInMinutes: "5"
        # if pull request, copy the cache to the tests folder to be used in the next steps
        condition: eq(variables['Build.Reason'], 'PullRequest')
        continueOnError: "true"
        displayName: "Copy/Unzip cached refs"

      - bash: |
          wget https://pl-public-data.s3.amazonaws.com/metrics/data.zip
          unzip -o data.zip
          ls -l _data/*
        workingDirectory: "tests/"
        displayName: "Pull testing data from S3"

      - bash: |
          du -h --max-depth=1 .
          python -m pytest $(TEST_DIRS) \
            -m "not DDP" --numprocesses=5 --dist=loadfile \
            --cov=torchmetrics --timeout=240 --durations=100 \
            --reruns 3 --reruns-delay 1
        workingDirectory: "tests/"
        # skip for PR if there is nothing to test, note that outside PR there is default 'unittests'
        condition: and(succeeded(), ne(variables['TEST_DIRS'], ''))
        timeoutInMinutes: "95"
        displayName: "UnitTesting common"

      - bash: |
          python -m pytest $(TEST_DIRS) -v \
            --cov=torchmetrics -m "DDP" \
            --timeout=240 --durations=100
        env:
          USE_PYTEST_POOL: "1"
        workingDirectory: "tests/"
        # skip for PR if there is nothing to test, note that outside PR there is default 'unittests'
        condition: and(succeeded(), ne(variables['TEST_DIRS'], ''))
        timeoutInMinutes: "95"
        displayName: "UnitTesting DDP"

      - bash: |
          du -h --max-depth=1 tests/
          # archive potentially updated cache to the machine filesystem to be reused with next jobs
          zip -q -r $(CACHED_REFERENCES) tests/_cache-references
          ls -lh $(CACHED_REFERENCES)
        # set as extra step to not pollute general cache when jobs fails or crashes
        # so do this update only with successful jobs on master
        condition: and(succeeded(), ne(variables['Build.Reason'], 'PullRequest'))
        displayName: "Update cached refs"

      - bash: |
          rm -rf tests/_cache-references/
        displayName: "Purge cached refs"

      - bash: |
          python -m coverage report
          python -m coverage xml
          python -m codecov --token=$(CODECOV_TOKEN) --name="GPU-coverage" \
            --commit=$(Build.SourceVersion) --flags=gpu,unittest --env=linux,azure
        workingDirectory: "tests/"
        # skip for PR if there is nothing to test, note that outside PR there is default 'unittests'
        condition: and(succeeded(), ne(variables['TEST_DIRS'], ''))
        displayName: "Statistics"

      - bash: |
          set -e
          FILES="*.py"
          for fn in $FILES
          do
            echo "Processing $fn example..."
            python $fn
          done
        workingDirectory: "_samples/"
        # skip for PR if there is nothing to test, note that outside PR there is default 'unittests'
        condition: and(succeeded(), ne(variables['TEST_DIRS'], ''))
        displayName: "Examples"

      - bash: |
          printf "cache location: $(HF_HOME)\n"
          ls -lh $(HF_HOME)  # show what was restored...
        # do not fail if the cache is not present
        continueOnError: "true"
        displayName: "Show HF artifacts"
