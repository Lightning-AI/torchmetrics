@misc{reproducibility,
    title={Enabling End-To-End Machine Learning Replicability: A Case Study in Educational Data Mining},
    author={Josh Gardner and Yuming Yang and Ryan Baker and Christopher Brooks},
    year={2018},
    eprint={1806.05208},
    archivePrefix={arXiv},
    primaryClass={cs.CY}
}

@inproceedings{fid,
    title={GANs Trained by a Two Time-Scale Update Rule Converge to a Local {N}ash Equilibrium},
    volume = {30},
    url = {https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html},
    booktitle = {Advances in Neural Information Processing Systems},
    publisher = {Curran Associates, Inc.},
    author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
    pages={6629--6640},
    year = {2017},
}

@misc{papers_with_code,
    title = {Papers With Code},
    author = {},
    howpublished = {\url{https://paperswithcode.com/}},
    note = {Accessed: 2021-12-01}
}

@misc{arxiv,
    title = {Arxiv},
    author = {},
    howpublished = {\url{https://arxiv.org/}},
    note = {Accessed: 2021-12-16}
}

@incollection{pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    booktitle = {Advances in Neural Information Processing Systems 32},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {8024--8035},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url={http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@article{scikit_learn,
    title={Scikit-learn: Machine Learning in {P}ython},
    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
    journal={Journal of Machine Learning Research},
    volume={12},
    pages={2825--2830},
    year={2011}
}

@misc{keras,
    title={Keras},
    author={Chollet, François and others},
    year={2015},
    publisher={GitHub},
    howpublished={\url{https://github.com/fchollet/keras}},
}

@misc{large_example1,
    title={Scaling Vision Transformers},
    author={Xiaohua Zhai and Alexander Kolesnikov and Neil Houlsby and Lucas Beyer},
    year={2021},
    eprint={2106.04560},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{large_example2,
    title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year={2019},
    eprint={1907.11692},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
